Principle of Least Perplexity

Here's a simple "intelligence design" principle: construct next-token predictor learning curricula* by presenting samples from lowest to highest perplexity. With a good perplexity estimator, I conjecture that recursive application of this rule can robustly outperform the absence of its application and enable smaller language models to do things today considered far beyond their capacity.

*Including, but not limited to, pretraining, fine-tuning, and/or in-context learning curricula, with perplexity grouped by window size, minibatch, epoch, and/or hand-assigned labels.

Inspirations:
[1] https://arxiv.org/abs/2101.10382
[2] https://arxiv.org/abs/2206.14486
[3] https://arxiv.org/abs/0812.4360
[4] https://www.crain.dev/3.txt

P.S.
It seems intuitive that the precision of the perplexity estimator matters more as the number of available samples increases.

P.P.S.
This principle is about learning good representations of data, not the goodness of the data represented, but applying this principle to a Kolmogorov pile [4] could help us to learn the best representation of everything.
