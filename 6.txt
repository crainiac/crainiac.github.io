Markov Blanket Weaving

Since autoregressive next-token predictor (antp) training is O(inference), using inference to augment training data seems like a promising area of applied research. I'm particularly interested in augmenting training data with representations of what the model has learned to represent; combined with the original data from which the model learned the representations, synthetic training data of this nature may possess desirable regularization properties.

One way to do this in a curriculum for learning sequence S:
Every N epochs,
[1] Recursively construct aperiodic sequences (or a tree; see below) Smin and Smax by greedily sampling the least and most perplexing tokens with sequence periodicity as a base case.
[2] Reverse Smin and Smax to obtain synthetic training sequences Zmin and Zmax.
[3] Reconstruct the curriculum sequence as Zmin, Zmax, S.

In practice, antps can generate multiple token probabilities in parallel (see, e.g., [4]), so any algorithm implementing this curriculum will probably give us all context-filling minimax paths for free. It's worth daydreaming about what we can do with these minimax perplexity trees and their potential connections to Nash equilibria and Markov boundaries.

Inspirations:
[1] https://en.wikipedia.org/wiki/Markov_blanket
[2] https://en.wikipedia.org/wiki/Kolmogorov%27s_criterion
[3] https://www.crain.dev/4.txt
[4] https://twitter.com/karpathy/status/1697318534555336961
